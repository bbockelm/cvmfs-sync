#!/usr/bin/python

import os
import pwd
import sys
import json
import time
import errno
import shutil
import urllib
import classad
import htcondor
import optparse
import urlparse

g_datasvc = "https://cmsweb.cern.ch/phedex/datasvc"
g_instance = "prod"
g_type = "json"

g_acct_grp = "cms.other.t3"

g_cvmfs = '/cvmfs/cms.osgstorage.org'

def parse_opts():
    usage = '%prog [options] tmpdir'
    parser = optparse.OptionParser(usage=usage)
    parser.add_option("-s", "--sites", dest="sites",
                      help="Sites to crawl for CMS data.")
    parser.add_option("-g", "--gigabytes", dest="gigabytes", default=100,
                      help="Approximate job size in GB.", type="int")
    parser.add_option("--dryrun", dest="dryrun", action="store_true",
                      help="Dry-run; do not actually submit jobs",
                      default=False)
    parser.add_option("-b", "--base", default="root://srm.unl.edu",
                      help="Base URL for path", dest="base")
    parser.add_option("-d", "--dataset", help="Publish a specific dataset",
                      dest="dataset")
    parser.add_option("-p", "--publish", default=False, action="store_true",
                      help="Publish prior run results to CVMFS", dest="publish")
    parser.add_option("--publish-only", default=False, action="store_true",
                      help="Only publish; do not submit jobs", dest="publish_only")
    opts, args = parser.parse_args()

    if len(args) != 1:
        parser.print_usage()
        sys.exit(1)

    sites = opts.sites.split()
    if sites == None:
        print >> sys.stderr, "Sites must be specified"
        parser.print_usage()
        sys.exit(1)
    opts.sites = opts.sites.split(",")

    return opts, args


def submit_job(schedd, job, dest):
    working_dir = os.path.abspath(os.path.join(dest, "%s_logs" % job))
    try:
        os.makedirs(working_dir)
    except OSError, oe:
        if oe.errno != errno.EEXIST:
            raise
    #print "Submitting job into working dir %s" % working_dir
    pw = pwd.getpwuid(os.geteuid())
    x509 = os.environ.get('X509_USER_PROXY', "/tmp/x509up_u%d" % pw.pw_uid)
    arg_list = ["-n", "10", "-f", "failed", os.path.split(job)[1], "."]
    arg_str = classad.Function("listToArgs", arg_list)
    in_files = ["cvmfs_swissknife", "libtbb_cvmfs.so.2", "libtbbmalloc_cvmfs.so.2", "pyxrootd", "XRootD", job]
    in_files = ", ".join([os.path.abspath(i) for i in in_files])
    ad = {'Arguments': arg_str,
          'Cmd': os.path.abspath("./cvmfs-file-sync"),
          'Environment': 'PATH=.:/usr/bin:/bin LD_LIBRARY_PATH=. PYTHONUNBUFFERED=1 XRD_DEBUG=Debug',
          'TransferInput': in_files,
          'TransferOutput': ",".join(["store", "failed"]),
          'TransferOutputRemaps': "out %(dir)s/out; err %(dir)s/err" % {"dir" : working_dir},
          'Out': "out",
          'Err': "err",
          'UserLog': os.path.abspath(os.path.join(dest, "condor_log")),
          'Iwd': dest,
          'AccountingGroup': "%s.%s" % (g_acct_grp, pw.pw_name),
          'x509userproxy': x509,
          'RequestMemory': 1024,
         }
    cluster = schedd.submit(ad)


def get_json(sites, dataset, dest):
    cache_dir = os.path.join(dest, "dbs_cache")
    try:
        os.makedirs(cache_dir)
    except OSError, oe:
        if oe.errno != errno.EEXIST:
            raise

    if dataset:
        return get_single_json(sites, [dataset])
    base_url = g_datasvc + "/" + g_type + "/" + g_instance + "/blockreplicasummary"
    urlparts = urlparse.urlparse(base_url)
    urlparts = list(urlparts)
    query = urlparse.parse_qsl(urlparts[4])
    for site in sites:
        query.append(("node", site))
    query.append(("dist_complete", "y"))
    query.append(("complete", "y"))
    urlparts[4] = urllib.urlencode(query)
    url = urlparse.urlunparse(urlparts)
    print "Querying", url

    fd = urllib.urlopen(url)

    datasets = {}
    for block in json.load(fd)[u'phedex']['block']:
        dataset = block['name'].split("#", 1)[0]
        block_set = datasets.setdefault(dataset, set())
        block_set.add(str(block['name']))

    final_results = {"block": []}

    missing_datasets = []
    for dataset in datasets:
        cache_fname = os.path.join(cache_dir + dataset, "filereplicas.json")
        used_cache = False
        if os.path.exists(cache_fname):
            block_info = json.load(open(cache_fname))
            cached_blocks = set([str(i['name']) for i in block_info['block']])
            if not cached_blocks.symmetric_difference(datasets[dataset]):
                final_results['block'] += block_info['block']
                used_cache = True
        if not used_cache:
            missing_datasets.append(dataset)

    print "There are %d datasets to query" % len(missing_datasets)
    idx = 0
    while missing_datasets:
        cur_datasets = missing_datasets[:20]
        missing_datasets = missing_datasets[20:]
        partial_results = get_single_json(sites, cur_datasets)
        idx += len(cur_datasets)
        print "%s: %d dataset queries complete." % (time.asctime(), idx)
        to_cache_datasets = {}
        for block in partial_results['block']:
             dataset = block['name'].split("#", 1)[0]
             block_list = to_cache_datasets.setdefault(dataset, [])
             block_list.append(block)
        for dataset, blocks in to_cache_datasets.items():
            cache_info = {"block": []}
            for block in blocks:
                cache_info['block'].append(block)
            dataset_dir = cache_dir + dataset
            try:
                os.makedirs(dataset_dir)
            except OSError, oe:
                if oe.errno != errno.EEXIST:
                    raise
            cache_fname = os.path.join(dataset_dir, "filereplicas.json")
            fd = open(cache_fname, "w")
            json.dump(cache_info, fd)
        final_results['block'] += partial_results['block']

    return final_results


def get_single_json(sites, datasets):
    base_url = g_datasvc + "/" + g_type + "/" + g_instance + "/filereplicas"
    urlparts = urlparse.urlparse(base_url)
    urlparts = list(urlparts)
    query = urlparse.parse_qsl(urlparts[4])
    for site in sites:
        query.append(("node", site))
    for dataset in datasets:
        query.append(("block", dataset + "#*"))
    urlparts[4] = urllib.urlencode(query)
    url = urlparse.urlunparse(urlparts)
    print "Querying", url

    fd = urllib.urlopen(url)
    return json.load(fd)[u'phedex']


def get_files(info):
    for block in info[u'block']:
        for file in block['file']:
            yield str(file['name']), file['bytes']


def do_publish(info, dest):
    known_replicas = dict(get_files(info))
    start_txn = False
    publish_dir = os.path.join(dest, "store")
    if not os.path.exists(publish_dir):
        print "Output directory %s does not exist; skipping publish." % publish_dir
        return
    print "Walking directory %s." % publish_dir
    for root, dirs, files in os.walk(os.path.join(dest, "store")):
        for fname in files:
            if not fname.startswith(".cvmfsgraft-"):
                continue
            real_fname = fname[len(".cvmfsgraft-"):]
            lfn = os.path.join(root, real_fname)[len(dest):]
            csum_size = -1
            for line in open(os.path.join(root, fname)):
                if line.startswith("size="):
                    csum_size = int(line[len("size="):])
                    break
            cvmfs_pfn = os.path.join(g_cvmfs + lfn)
            if csum_size == -1:
                print "LFN %s missing size in graft file.  Skipping and deleting the bad graft." % lfn
                os.unlink(os.path.join(root, fname))
                try:
                    os.unlink(os.path.join(root, real_fname))
                except OSError, oe:
                    if oe.errno != errno.ENOENT:
                        raise
                continue
            elif lfn not in known_replicas:
                print "LFN %s has a checksum file but not in known replicas.  Skipping" % lfn
            elif known_replicas[lfn] != csum_size:
                print "LFN %s has local file size %d but PhEDEx size %d.  Skipping and deleting the bad graft." % (lfn, csum_size, known_replicas[lfn])
                os.unlink(os.path.join(root, fname))
                try:
                    os.unlink(os.path.join(root, real_fname))
                except OSError, oe:
                    if oe.errno != errno.ENOENT:
                        raise
            elif os.path.exists(cvmfs_pfn) and (os.stat(cvmfs_pfn).st_size == csum_size):
                print "Ignoring graft for LFN %s as it already exists in CVMFS." % lfn
                os.unlink(os.path.join(root, fname))
                try:
                    os.unlink(os.path.join(root, real_fname))
                except OSError, oe:
                    if oe.errno != errno.ENOENT:
                        raise
                continue
            else:
                if not start_txn:
                    start_txn = True
                    result = os.system("cvmfs_server transaction %s" % os.path.split(g_cvmfs)[-1])
                    if result:
                        raise Exception("Start of transaction for %s failed with %d" % (os.path.split(g_cvmfs)[-1], result))
                lfn_dir = os.path.split(lfn)[0]
                print "Publishing", lfn
                try:
                    os.makedirs(g_cvmfs + lfn_dir)
                except OSError, oe:
                    if oe.errno != errno.EEXIST:
                        raise
                shutil.move(os.path.join(root, fname), os.path.join(g_cvmfs + lfn_dir, fname))
                shutil.move(os.path.join(root, real_fname), os.path.join(g_cvmfs + lfn))

    if start_txn:
        result = os.system("cvmfs_server publish -X %s" % os.path.split(g_cvmfs)[-1])
    else:
        print "No files to publish."


def main():
    opts, args = parse_opts()

    dest = os.path.abspath(args[0])
    try:
        os.makedirs(dest)
    except OSError, oe:
        if oe.errno != errno.EEXIST:
            raise

    info = get_json(opts.sites, opts.dataset, dest)
    if opts.publish:
        do_publish(info, dest)

    if opts.publish_only:
        return 0

    sum_size = 0
    files = []
    count = 0

    offset = 0
    while True:
        if not os.path.exists(os.path.join(dest, "job_%d" % offset)):
            break
        offset += 1

    for file, size in get_files(info):
        cvmfs_name = g_cvmfs + file
        try:
            st = os.stat(cvmfs_name)
            if st.st_size != size:
                print "File %s exists but wrong size; re-generating" % file
            else:
                # Already there, skip!
                continue
        except OSError, oe:
            if oe.errno != errno.ENOENT:
                raise
        files.append(opts.base + "/" + file)
        sum_size += size
        if sum_size >= opts.gigabytes*1e9:
            fd = open(os.path.join(dest, "job_%d" % (count + offset)), "w")
            fd.write("\n".join(files) + "\n")
            files = []
            sum_size = 0
            count += 1

    if not opts.dryrun:
        schedd = htcondor.Schedd()
        print "Submitting %d jobs" % count
        with schedd.transaction() as txn:
            for idx in range(count):
                name = os.path.join(dest, "job_%d" % (idx + offset))
                submit_job(schedd, name, dest)
    else:
        print "There are %d jobs to submit into directory %s." % (count, dest)


    return 0


if __name__ == '__main__':
    sys.exit(main())

