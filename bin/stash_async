#!/usr/bin/python

"""
Rewritten version of stash-sync.  Features:
- Directory processing utilizes generators.  This way,
  we can start processing files while we crawl directories.
- Avoid use of asynchronous callbacks, which have a tendency to
  deadlock.  Python subprocesses are used instead.
"""

import os
import re
import sys
import time
import errno
import Queue
import random
import signal
import optparse
import urlparse
import threading
import traceback
import multiprocessing

oldpath = sys.path
sys.path = sys.path[1:]

import XRootD.client

sys.path = oldpath

g_processed_files = []
g_bytes_xfer = 0
g_bytes_xfer_lock = threading.Lock()

g_starttime = time.time()

def parse_opts():
    parser = optparse.OptionParser(usage="%prog [options] src1 [src2 ...] dest")
    parser.add_option("-n", "--concurrency", dest="concurrency",
                       help="Number of helper processes", default=1,
                       type="int")
    parser.add_option("-f", "--failed", dest="failed",
                      help="Output filename for list of failed files")
    parser.add_option("-t", "--max-time", dest="max_time", type="int",
                      help="Time threshold for starting new batches "
                           " of transfers.  Specified in minutes",
                      default = 0)
    parser.add_option("-d", "--delete", dest="delete", action="store_true",
                      help="Delete files from CVMFS if they are not present in Stash")

    opts, args = parser.parse_args()
    if len(args) < 2:
        print >> sys.stderr, "Need at least one source and destination"
        parser.print_usage()
        sys.exit(1)

    return opts, args


def kickoff_graft(fp, input_url, output_filename, deadline):

    if (deadline > 0) and (time.time() > deadline):
        return

    print "Grafting", input_url, "to", output_filename
    readfp, writefp = os.pipe()
    pid = os.fork()
    if not pid:
        try:
            os.dup2(readfp, 0)
            os.close(writefp)
            args = ['cvmfs_swissknife', 'graft', '-i', '-', '-o', output_filename, '-Z', 'none']
            os.execvp("cvmfs_swissknife", args)
        finally:
            print "Failed to exec cvmfs_swissknife for processing", input_url
            os._exit(1)
    os.close(readfp)
    next_offset = 0
    while True:
        if (deadline > 0) and (time.time() > deadline):
            print "Timed out when transferring", input_url
            os.kill(pid, signal.SIGKILL)
            os.waitpid(pid, 0)
            return False

        status, response = fp.read(offset=next_offset, size=1024*1024)
        if status.status:
            print "Failed to create read request for", input_url
            print "Status:", status
            print "Killing graft PID", pid
            os.kill(pid, signal.SIGKILL)
            os.waitpid(pid, 0)
            return False
        # Empty response indicates EOF.
        if (response == None) or (len(response) == 0):
            print "Finished processing", input_url 
            os.close(writefp)
            os.waitpid(pid, 0)
            break
        else:
            try:
                os.write(writefp, response)
            except OSError, oe:
                if oe.errno != errno.EPIPE:
                    raise
                print "cvmfs_swissknife process unexpectedly died."
                return False
            next_offset += len(response)

    return True


def process_single_file(filename, size, base_url, output_dir, deadline):
    if (deadline > 0) and (time.time() > deadline):
        return

    current = 0

    # Serialize on opening files
    input_url = base_url + filename
    output_filename = output_dir + '/' + filename
    if os.path.exists(output_filename):
        st = os.stat(output_filename)
        if size == st.st_size:
            #print "Skipping file %s as %s already exists." % \
            #    (input_url, output_filename)
            return
        else:
            print "Size mismatch!  Regenerating file %s (size %d) -> %s (size %d)" % \
                    (input_url, size, output_filename, st.st_size)
            os.unlink(output_filename)
    try:
        os.makedirs(os.path.split(output_filename)[0])
    except OSError, oe:
        if oe.errno != errno.EEXIST:
            raise
    fp = XRootD.client.File()

    status = fp.open(input_url)[0]
    if status.status:
        print "Failed to open file %s:" % input_url, status
        return filename

    input_url = base_url + filename
    output_filename = output_dir + '/' + filename
    if not kickoff_graft(fp, input_url, output_filename, deadline):
        return filename


def list_dir(work_queue, results_queue, fs):

    while True:
        cwd = work_queue.get()
        print "Begin dirlist of", cwd
        status = None
        dirlist = None
        try:
            status, dirlist = fs.dirlist(cwd,
                                        flags=XRootD.client.flags.DirListFlags.STAT)
        finally:
            results_queue.put((status, dirlist, cwd), True)
            work_queue.task_done()


user_dir_regexp = re.compile(r'^/+user/+([A-Za-z0-9.]+)/*$')
def process_dir(base_url, directory, thread_count):
    fs = XRootD.client.FileSystem(base_url)
    worklist = [directory]
    base_len = len(directory)

    work_queue = Queue.Queue(maxsize=thread_count)
    work_sent = 0
    results_queue = Queue.Queue() # shouldn't block
    results_received = 0
    for count in range(thread_count):
        t = threading.Thread(target=list_dir, args=(work_queue, results_queue, fs), name="Directory list worker %d" % count)
        t.setDaemon(True)
        t.start()

    while worklist or (work_sent != results_received):

        while worklist and not work_queue.full():
            cwd = worklist.pop()
            slash_re = re.compile("/+")
            cwd = slash_re.subn("/", cwd)[0]
            if user_dir_regexp.match(cwd):
                cwd += "/public"
            # Upstream server has issues with these directories.
            if cwd.startswith('/user/yadunand') or cwd.startswith('/user/ndh8') or cwd.startswith('/user/vpentchev/public/tutorial-R/result'):
                yield '/' + cwd, -1, True
                continue
            work_sent += 1
            work_queue.put("/" + cwd, True)

        status, dirlist, cwd = results_queue.get(True)
        print "Processing entries of", cwd
        results_queue.task_done()
        results_received += 1
        if (status is None) and (dirlist is None):
            print "Failure when processing dir:", cwd
            yield '/' + cwd, -1, True
            continue
        if status.status:
            print "Failed to list directory:", cwd
            yield '/' + cwd, -1, True
            continue
        # Shuffle processing result - reduces the likelihood we get "stuck" in one directory.
        random_dirlist = list(dirlist.dirlist)
        random.shuffle(random_dirlist)
        for entry in random_dirlist:
            if entry.statinfo.flags & XRootD.client.flags.StatInfoFlags.IS_DIR:
                # Breadth-first-search: as close as we get to fairsharing between user dirs.
                worklist.insert(0, cwd + "/" + entry.name)
            else:
                fname = cwd + "/" + entry.name
                yield fname[base_len:], entry.statinfo.size, False


def process_deletions(base_dir, orig_filelist, failed, dryrun):
    slash_re = re.compile("/+")
    filelist = []
    for fname in orig_filelist:
        try:
            filelist.append(slash_re.subn("/", base_dir + "/" + fname)[0])
        except TypeError:
            print fname
            raise
    filelist = set(filelist)
    dirlist = set([base_dir])
    for fname in filelist:
        dirname = os.path.split(fname)[0]
        while dirname not in dirlist:
            dirlist.add(dirname)
            dirname = os.path.split(dirname)[0]
    failed = set([slash_re.subn("/", i)[0] for i in failed])
    count = 0
    for dirpath, dirs, files in os.walk(base_dir):
        dirpath = slash_re.sub("/", "/" + dirpath)
        found_bad = False
        for badpath in failed:
            if dirpath.startswith(badpath):
                found_bad = True
                break
        if found_bad:
            continue
        for fname in files:
            if fname.startswith(".cvmfs"):
                continue
            full_fname = os.path.join(dirpath, fname)
            if full_fname not in filelist:
                count += 1
                if dryrun:
                    print "Would remove file %s." % full_fname
                else:
                    os.unlink(full_fname)
        for dname in dirs:
            full_dname = os.path.join(dirpath, dname)
            if full_dname not in dirlist:
                if dryrun:
                    print "Would remove directory %s." % full_dname
                else:
                    # OverlayFS returns errant ENOTEMPTY on RHEL7.2
                    try:
                        os.rmdir(full_dname)
                    except OSError, oe:
                        if os.errno != errno.ENOTEMPTY:
                            raise
    if count and dryrun:
        print "Would have removed %d files." % count


def scan_futures(futures, full_failed_list, maxwait):
    random.shuffle(futures)
    drain_deadline = time.time() + maxwait
    new_futures = []
    # First, iterate through immediately-finished ones.
    for future in futures:
        if maxwait > 0:
            remaining = drain_deadline - time.time()
            if remaining > 0:
                future.wait(remaining)
        if future.ready():
            try:
                result = future.get()
                if result:
                    full_failed_list.append(result)
            except:
                traceback.print_exc()
        else:
            new_futures.append(future)
    return new_futures, full_failed_list


def main():
    opts, args = parse_opts()
    dest = args[-1]
    srcs = args[:-1]
    print "Inputs to process:", ",".join(srcs)
    concurrency = opts.concurrency
    starttime = time.time()
    if opts.max_time > 0:
        deadline = opts.max_time*60 + starttime
    else:
        deadline = starttime + 86400*7

    thread_count = 1

    pool = multiprocessing.Pool(processes=concurrency)

    full_filelist = []
    full_failed_list = []
    futures = []
    for src in srcs:
        if time.time() > deadline:
            break
        src_parsed = urlparse.urlparse(src)
        base_url = src_parsed.scheme + "://" + src_parsed.netloc
        directory = src_parsed.path
        for filename, size, failed in process_dir(base_url, directory, thread_count):
            if time.time() > deadline:
                break
            if failed:
                full_failed_list.append(filename)
            else:
                full_filelist.append(filename)
                future = pool.apply_async(process_single_file, (filename, size, src, dest, deadline))
                futures.append(future)

            count = 0
            while len(futures) >= 1000:
                futures, full_failed_list = scan_futures(futures, full_failed_list, 0)
                sleep_time = min(5, .1 * 2**count)
                count += 1
                time.sleep(count)
        print "Found %d files to process" % len(full_filelist)

    pool.close()
    pool.join()

    processing_time = time.time() - starttime
    print "Total of %d bytes processed in %.2f seconds." % (g_bytes_xfer, processing_time)
    print "Processing rate: %.2fMB/s" % (g_bytes_xfer/processing_time/(1024*1024.))

    dryrun = False
    if opts.delete or dryrun:
        if deadline - 30 < time.time():
            print "Not processing deletions because we didn't have a full run"
        else:
            process_deletions(dest, full_filelist, full_failed_list, dryrun)

    if not opts.failed:
        if full_failed_list:
            print "Failed files:"
            for fname in full_failed_list:
                print fname
    else:
        fh = open(opts.failed, "w")
        fh.write("\n".join(full_failed_list))
    if not full_failed_list:
        print "Synchronization completed successfully!"
        return 0
    else:
        print "There were %d synchronization failures." % len(full_failed_list)
        # TODO: cleanup partial grafts
        return 1


if __name__ == '__main__':
    sys.exit(main())


