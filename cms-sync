#!/usr/bin/python

import os
import sys
import time
import errno
import random
import fnmatch
import optparse
import urlparse
import threading
import subprocess
import multiprocessing

oldpath = sys.path
sys.path = sys.path[1:]

import XRootD.client

sys.path = oldpath

g_failed_files = []
g_processed_files = []
g_bytes_xfer = 0
g_skip_count = 0

def inc_bytes(count):
    global g_bytes_xfer
    g_bytes_xfer_lock.acquire()
    try:
        g_bytes_xfer += count
    finally:
        g_bytes_xfer_lock.release()

def parse_opts():
    parser = optparse.OptionParser(usage="%prog [options] src1 [src2 ...] dest")
    parser.add_option("-n", "--concurrency", dest="concurrency",
                       help="Number of helper processes", default=4,
                       type="int")
    parser.add_option("-f", "--failed", dest="failed",
                      help="Output filename for list of failed files")
    parser.add_option("-i", "--ignore", dest="ignore",
                      help="Unix glob of filenames to ignore.")

    opts, args = parser.parse_args()
    if len(args) < 2:
        print >> sys.stderr, "Need at least one source and destination"
        parser.print_usage()
        sys.exit(1)

    return opts, args

def graft_filename(filename):
    parent_dir, fname = os.path.split(filename)
    return os.path.join(parent_dir, ".cvmfsgraft-" + fname)


def should_skip(input_url, output_filename, size):
    global g_skip_count
    graftfile = graft_filename(output_filename)

    # CVMFS checksum format has trouble with very large files.
    # There's additionally suspicious-looking failures around zero-sized
    # files.
    if (size > 30*(1024**3)) or (size == 0):
        return True

    if os.path.exists(output_filename):
        st = os.stat(output_filename)
        if size == st.st_size:
            g_skip_count += 1
            return True
        elif os.path.exists(graftfile):
            graft_size = -1
            with open(graftfile, "r") as fp:
                for line in fp:
                    if line.startswith("size="):
                        try:
                            graft_size = int(line.strip().split("=", 1)[-1])
                            break
                        except:
                            pass
            if size == graft_size:
                g_skip_count += 1
                return True
            print "Graft size mismatch!  Regenerating file %s (size %d) -> %s (graft size %d)" % \
                    (input_url, size, output_filename, graft_size)
        else:
            print "Size mismatch!  Regenerating file %s (size %d) -> %s (size %d)" % \
                    (input_url, size, output_filename, st.st_size)
            os.unlink(output_filename)
    try:
        os.makedirs(os.path.split(output_filename)[0])
    except OSError, oe:
        if oe.errno != errno.EEXIST:
            raise
    return False

def process_checksum_file(gridftp_url, output_filename):
    parsed = urlparse.urlparse(gridftp_url)
    print "Querying GridFTP server for %s" % gridftp_url
    try:
        output = subprocess.check_output(["uberftp", parsed.netloc, "quote cksm cvmfs 0 -1 %s" % parsed.path])
    except subprocess.CalledProcessError as cpe:  # https://bugs.python.org/issue9400 - CalledProcessError messes up multiprocessing
        raise Exception("UberFTP call failed. (cmd=%s, output=%s, returncode=%s)" % (cpe.cmd, cpe.output, str(cpe.returncode)))
    cvmfs_cksum = None
    for line in output.splitlines():
        line = line.strip()
        if line.startswith("500"):
            raise Exception(line)
        elif line.startswith("213 "):
            cvmfs_cksum = line[4:]
    if cvmfs_cksum:
        with open(output_filename, "w") as fp:
            pass
        graftfile = cvmfs_cksum.replace(";", "\n")
        with open(graft_filename(output_filename), "w") as fp:
            fp.write(graftfile + "\n")
    else:
        raise Exception("Remote GridFTP server did not return a CVMFS checksum")


def process_files(base_url, gridftp_base_url, output_dir, count, ignore=None):
    global g_bytes_xfer

    current = 0
    worklist = []

    xrootd_base_parsed = urlparse.urlparse(base_url)
    xrootd_connection_info = xrootd_base_parsed.scheme + "://" + xrootd_base_parsed.netloc
    xrootd_directory = xrootd_base_parsed.path

    # First, try to retrieve the file
    pool = multiprocessing.Pool(count)
    futures = []
    for filename, size in process_dir(xrootd_connection_info, xrootd_directory, output_dir, ignore=ignore):
        if len(futures) >= 20000:
            print "Queue of 20k files to checksum has been reached; will attempt remainder next time."
            break
        gridftp_url = gridftp_base_url + filename
        output_filename = output_dir + filename
        input_url = base_url + filename
        if should_skip(input_url, output_filename, size):
            continue
        future = pool.apply_async(process_checksum_file, (gridftp_url, output_filename))
        future.filename = filename
        future.size = size
        futures.append(future)
        #break
    pool.close()
    while futures:
        timed_out_futures = []
        for future in futures:
            try:
                future.get(180)
                g_processed_files.append(future.filename)
                g_bytes_xfer += future.size
            except multiprocessing.TimeoutError:
                print "Timed out when waiting for file %s; will retry later." % future.filename
                #timed_out_futures.append(future)
            except Exception as e:
                print "Failure when trying to checksum %s (size %d); will regenerate.  Exception: %s" % (future.filename, future.size, str(e))
                g_failed_files.append(gridftp_base_url + "/" + future.filename)
        futures = timed_out_futures
    print "All jobs processed - closing up pool."
    pool.join()


def process_dir(base_url, directory, output_dir, ignore=None):
    fs = XRootD.client.FileSystem(base_url)
    worklist = [directory]
    base_len = len(directory)
    while worklist:
        cwd = worklist.pop()
        print "Processing", cwd
        status, dirlist = fs.dirlist("/" + cwd,
                                    flags=XRootD.client.flags.DirListFlags.STAT)
        if status.status:
            print "Failed to list directory", cwd, "skipping"
            print status
            continue
        xrootd_cwd_names = set()
        entries = list(dirlist.dirlist)
        random.shuffle(entries)
        for entry in entries:
            should_ignore = False
            if ignore:
                for ignore_rule in ignore.split(","):
                    if fnmatch.fnmatch(entry.name, ignore_rule):
                        should_ignore = True
                        break
            # Sometimes the remote XRootD will prefix the name with '/'
            xrootd_cwd_names.add(entry.name.split("/")[-1])
            if entry.statinfo.flags & XRootD.client.flags.StatInfoFlags.IS_DIR:
                worklist.append(cwd + "/" + entry.name)
            else:
                fname = cwd + "/" + entry.name
                yield (fname[base_len:], entry.statinfo.size)
        cwd_output = output_dir + "/" + cwd[len(directory):]
        try:
            cwd_output_names = set(os.listdir(cwd_output))
        except OSError as oe:
            if oe.errno == errno.ENOENT:
                print "Destination directory (%s) does not exist; not checking for removal candidates." % cwd_output
                continue
        remaining_names = cwd_output_names.difference(xrootd_cwd_names)
        for name in remaining_names:
            # Ignore presence of hidden files (such as the graft files).
            if name.startswith("."): continue
            fname = os.path.join(cwd_output, name)
            print "Candidate for removal:", fname
            try:
                os.unlink(fname)
            except OSError as oe:
                print "Failed to remove file %s: %s" % (fname, str(oe))

def main():
    opts, args = parse_opts()
    dest = args[-1]
    srcs = args[:-1]
    print "Inputs to process:", ",".join(srcs)
    concurrency = opts.concurrency
    starttime = time.time()

    for src in srcs:
        source_filelist = []
        src, gridftp_src = src.split(",")
        process_files(src, gridftp_src, dest, concurrency, ignore=opts.ignore)

    processing_time = time.time() - starttime
    print "Total of %.1f GB in %d files processed in %.2f seconds." % (g_bytes_xfer/(1024.**3), len(g_processed_files), processing_time)
    print "%d file checksums were skipped because they already exist locally." % g_skip_count
    # Processing rate is not sensible as we don't know if this is a "new" checksum.
    #print "Processing rate: %.2fMB/s" % (g_bytes_xfer/processing_time/(1024*1024.))

    if not opts.failed:
        if g_failed_files:
            print "Failed files:"
            for file in g_failed_files:
                print file
    else:
        fh = open(opts.failed, "w")
        fh.write("\n".join(g_failed_files))
    if not g_failed_files:
        print "Synchronization completed successfully!"
        return 0
    else:
        print "There were %d synchronization failures." % len(g_failed_files)
        # TODO: cleanup partial grafts
        return 1


if __name__ == '__main__':
    sys.exit(main())


