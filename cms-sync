#!/usr/bin/python

import os
import sys
import time
import errno
import signal
import fnmatch
import optparse
import urlparse
import threading
import subprocess
import multiprocessing

oldpath = sys.path
sys.path = sys.path[1:]

import XRootD.client

sys.path = oldpath

g_failed_files = []
g_processed_files = []
g_bytes_xfer = 0
g_bytes_xfer_lock = threading.Lock()

signal.alarm(4*3600)

def inc_bytes(count):
    global g_bytes_xfer
    g_bytes_xfer_lock.acquire()
    try:
        g_bytes_xfer += count
    finally:
        g_bytes_xfer_lock.release()

def parse_opts():
    parser = optparse.OptionParser(usage="%prog [options] src1 [src2 ...] dest")
    parser.add_option("-n", "--concurrency", dest="concurrency",
                       help="Number of helper processes", default=4,
                       type="int")
    parser.add_option("-f", "--failed", dest="failed",
                      help="Output filename for list of failed files")
    parser.add_option("-i", "--ignore", dest="ignore",
                      help="Unix glob of filenames to ignore.")

    opts, args = parser.parse_args()
    if len(args) < 2:
        print >> sys.stderr, "Need at least one source and destination"
        parser.print_usage()
        sys.exit(1)

    return opts, args


def create_read_callback(offset, readfp, writefd, filename, pid):
    def read_callback(status, response, hostlist):
        # NOTE: due to mutex ordering issues, we CANNOT invoke xrootd
        # functions (including letting objects fall out of scope).
        #print "Read callback for %s, offset %d, response" % (filename, offset)
        if status.status:
            print "Read callback failed for %s:" % filename, status
            print "Hostlist:", hostlist
            for host in hostlist.hosts:
                print "-", host.url
            g_failed_files.append(filename)
            os.kill(pid, signal.SIGKILL)
            # In this case, response is None; close and fail out.
        if (response == None) or (len(response) == 0):
            print "Finished processing", filename
            os.close(writefd)
        else:
            inc_bytes(len(response))
            os.write(writefd, response)
            next_offset = offset + len(response)
            kwargs = {'offset': next_offset,
                      'size': 1024*1024,
                      'callback': create_read_callback(next_offset, readfp,
                                                writefd, filename, pid)
                     }
            status = readfp.read(**kwargs)
            if status.status:
                print "Failed to create callback for", filename
                g_failed_files.append(filename)
                os.kill(pid, signal.SIGKILL)

    return read_callback


def kickoff_graft(fp, input_url, output_filename):
    readfp, writefp = os.pipe()
    pid = os.fork()
    if not pid:
        try:
            os.dup2(readfp, 0)
            os.close(writefp)
            args = ['cvmfs_swissknife', 'graft', '-i', '-', '-o', output_filename, '-Z', 'none']
            os.execvp("cvmfs_swissknife", args)
        finally:
            print "Failed to exec cvmfs_swissknife for processing", input_url
            os._exit(1)
    os.close(readfp)
    callback = create_read_callback(0, fp, writefp, input_url, pid)
    status = fp.read(offset=0, size=1024*1024, callback=callback)
    if status.status:
        print "Failed to create read request for", input_url
        print "Status:", status
        print "Killing graft PID", pid
        os.kill(pid, signal.SIGKILL)
        g_failed_files.append(input_url)
        return False
    return True


def process_files(filenames, base_url, gridftp_base_url, output_dir, count):
    bulk_count = 10000
    while filenames:
        chunk_filenames = filenames[:bulk_count]
        filenames = filenames[bulk_count:]
        process_files_impl(chunk_filenames, base_url, gridftp_base_url, output_dir, count)


def should_skip(input_url, output_filename, size):
    if os.path.exists(output_filename):
        st = os.stat(output_filename)
        if size == st.st_size:
            return True
        else:
            print "Size mismatch!  Regenerating file %s (size %d) -> %s (size %d)" % \
                    (input_url, size, output_filename, st.st_size)
            os.unlink(output_filename)
    try:
        os.makedirs(os.path.split(output_filename)[0])
    except OSError, oe:
        if oe.errno != errno.EEXIST:
            raise
    return False

def process_checksum_file(gridftp_url, output_filename):
    parsed = urlparse.urlparse(gridftp_url)
    print "Querying GridFTP server for %s" % gridftp_url
    output = subprocess.check_output(["uberftp", parsed.netloc, "quote cksm cvmfs 0 -1 %s" % parsed.path])
    cvmfs_cksum = None
    for line in output.splitlines():
        line = line.strip()
        if line.startswith("500"):
            raise Exception(line)
        elif line.startswith("213 "):
            cvmfs_cksum = line[4:]
    if cvmfs_cksum:
        output_dir, output_fname = os.path.split(output_filename)
        with open(output_filename, "w") as fp:
            pass
        graftfile = cvmfs_cksum.replace(";", "\n")
        with open(os.path.join(output_dir, ".cvmfsgraft-" + output_fname), "w") as fp:
            fp.write(graftfile + "\n")
    else:
        raise Exception("Remote GridFTP server did not return a CVMFS checksum")
    #print output


def process_files_impl(filenames, base_url, gridftp_base_url, output_dir, count):
    current = 0
    worklist = []

    # First, try to retrieve the file
    broken_filenames = []
    pool = multiprocessing.Pool(count)
    futures = []
    for filename, size in filenames:
        gridftp_url = gridftp_base_url + filename
        output_filename = output_dir + filename
        input_url = base_url + filename
        if should_skip(input_url, output_filename, size):
            continue
        future = pool.apply_async(process_checksum_file, (gridftp_url, output_filename))
        future.filename = filename
        future.size = size
        futures.append(future)
        #break
    pool.close()
    for future in futures:
        try:
            future.get()
        except Exception as e:
            print "Failure when trying to checksum %s (size %d); will regenerate.  Exception: %s" % (future.filename, future.size, str(e))
            broken_filenames.append((future.filename, future.size))
    print "All jobs processed - closing up pool."
    pool.join()
    filenames = broken_filenames

    # Serialize on opening files
    for filename, size in filenames:
        input_url = base_url + filename
        output_filename = output_dir + filename
        if should_skip(input_url, output_filename, size):
            continue
        fp = XRootD.client.File()
        status = fp.open(input_url)[0]
        if status.status:
            g_failed_files.append(input_url)
            print "Failed to open file %s:" % input_url, status
        else:
            worklist.append((filename, fp))

    # Process in parallel.
    try:
        for filename, fp in worklist:
            input_url = base_url + filename
            output_filename = output_dir + filename
            if current == count:
                status = os.wait()[1]
                current -= 1
                if status:
                    print "Failed to process file; graft creation exited with error", status
            print "Processing file %s -> %s" % (input_url, output_filename)
            kickoff_graft(fp, input_url, output_filename)
            current += 1
    finally:
        # Process tails - must do this before exceptions are propagated as we
        # may deadlock if there are callbacks while we destroy the open file
        # handles.
        while current:
            status = os.wait()[1]
            current -= 1
            if status:
                print "Failed to process file; graft creation exited with error", status
        worklist = []
        # It's not possible to solve the race condition inherent here - we have no
        # way to notify the main thread the callback pool is done without holding
        # the GIL.  So, we do this any pray.
        time.sleep(1)


def process_dir(base_url, directory, ignore=None):
    fs = XRootD.client.FileSystem(base_url)
    worklist = [directory]
    filelist = []
    base_len = len(directory)
    while worklist:
        cwd = worklist.pop()
        print "Processing", cwd
        status, dirlist = fs.dirlist("/" + cwd,
                                    flags=XRootD.client.flags.DirListFlags.STAT)
        if status.status:
            print "Failed to list directory", cwd, "skipping"
            print status
            continue
        for entry in dirlist.dirlist:
            if entry.statinfo.flags & XRootD.client.flags.StatInfoFlags.IS_DIR:
                should_ignore = False
                if ignore:
                    for ignore_rule in ignore.split(","):
                        if fnmatch.fnmatch(entry.name, ignore_rule):
                            should_ignore = True
                            break
                if not should_ignore:
                    worklist.append(cwd + "/" + entry.name)
            elif not ignore:
                fname = cwd + "/" + entry.name
                filelist.append((fname[base_len:], entry.statinfo.size))
            else:
                should_ignore = False
                if ignore:
                    for ignore_rule in ignore.split(","):
                         if fnmatch.fnmatch(entry.name, ignore_rule):
                             should_ignore = True
                             break
                if not should_ignore:
                    fname = cwd + "/" + entry.name
                    filelist.append((fname[base_len:], entry.statinfo.size))
    return filelist


def main():
    opts, args = parse_opts()
    dest = args[-1]
    srcs = args[:-1]
    print "Inputs to process:", ",".join(srcs)
    concurrency = opts.concurrency
    starttime = time.time()

    for src in srcs:
        source_filelist = []
        src, gridftp_src = src.split(",")
        src_parsed = urlparse.urlparse(src)
        base_url = src_parsed.scheme + "://" + src_parsed.netloc
        directory = src_parsed.path
        filelist = process_dir(base_url, directory, ignore=opts.ignore)
        print "Found %d files to process" % len(filelist)
        process_files(filelist, src, gridftp_src, dest, concurrency)

    processing_time = time.time() - starttime
    print "Total of %d bytes processed in %.2f seconds." % (g_bytes_xfer, processing_time)
    print "Processing rate: %.2fMB/s" % (g_bytes_xfer/processing_time/(1024*1024.))

    if not opts.failed:
        if g_failed_files:
            print "Failed files:"
            for file in g_failed_files:
                print file
    else:
        fh = open(opts.failed, "w")
        fh.write("\n".join(g_failed_files))
    if not g_failed_files:
        print "Synchronization completed successfully!"
        return 0
    else:
        print "There were %d synchronization failures." % len(g_failed_files)
        # TODO: cleanup partial grafts
        return 1


if __name__ == '__main__':
    sys.exit(main())


