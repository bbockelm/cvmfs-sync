#!/usr/bin/python

import os
import sys
import time
import errno
import fnmatch
import optparse
import urlparse
import threading
import subprocess
import multiprocessing

oldpath = sys.path
sys.path = sys.path[1:]

import XRootD.client

sys.path = oldpath

g_failed_files = []
g_processed_files = []
g_bytes_xfer = 0
g_bytes_xfer_lock = threading.Lock()

def inc_bytes(count):
    global g_bytes_xfer
    g_bytes_xfer_lock.acquire()
    try:
        g_bytes_xfer += count
    finally:
        g_bytes_xfer_lock.release()

def parse_opts():
    parser = optparse.OptionParser(usage="%prog [options] src1 [src2 ...] dest")
    parser.add_option("-n", "--concurrency", dest="concurrency",
                       help="Number of helper processes", default=4,
                       type="int")
    parser.add_option("-f", "--failed", dest="failed",
                      help="Output filename for list of failed files")
    parser.add_option("-i", "--ignore", dest="ignore",
                      help="Unix glob of filenames to ignore.")

    opts, args = parser.parse_args()
    if len(args) < 2:
        print >> sys.stderr, "Need at least one source and destination"
        parser.print_usage()
        sys.exit(1)

    return opts, args


def should_skip(input_url, output_filename, size):
    if os.path.exists(output_filename):
        st = os.stat(output_filename)
        if size == st.st_size:
            return True
        else:
            print "Size mismatch!  Regenerating file %s (size %d) -> %s (size %d)" % \
                    (input_url, size, output_filename, st.st_size)
            os.unlink(output_filename)
    try:
        os.makedirs(os.path.split(output_filename)[0])
    except OSError, oe:
        if oe.errno != errno.EEXIST:
            raise
    return False

def process_checksum_file(gridftp_url, output_filename):
    parsed = urlparse.urlparse(gridftp_url)
    print "Querying GridFTP server for %s" % gridftp_url
    try:
        output = subprocess.check_output(["uberftp", parsed.netloc, "quote cksm cvmfs 0 -1 %s" % parsed.path])
    except subprocess.CalledProcessError as cpe:  # https://bugs.python.org/issue9400 - CalledProcessError messes up multiprocessing
        raise Exception("UberFTP call failed. (cmd=%s, output=%s, returncode=%s)" % (cpe.cmd, cpe.output, str(cpe.returncode)))
    cvmfs_cksum = None
    for line in output.splitlines():
        line = line.strip()
        if line.startswith("500"):
            raise Exception(line)
        elif line.startswith("213 "):
            cvmfs_cksum = line[4:]
    if cvmfs_cksum:
        output_dir, output_fname = os.path.split(output_filename)
        with open(output_filename, "w") as fp:
            pass
        graftfile = cvmfs_cksum.replace(";", "\n")
        with open(os.path.join(output_dir, ".cvmfsgraft-" + output_fname), "w") as fp:
            fp.write(graftfile + "\n")
    else:
        raise Exception("Remote GridFTP server did not return a CVMFS checksum")


def process_files(filenames, base_url, gridftp_base_url, output_dir, count):
    current = 0
    worklist = []

    # First, try to retrieve the file
    broken_filenames = []
    pool = multiprocessing.Pool(count)
    futures = []
    for filename, size in filenames:
        if len(futures) >= 20000:
            print "Queue of 20k files to checksum has been reached; will attempt remainder next time."
            break
        gridftp_url = gridftp_base_url + filename
        output_filename = output_dir + filename
        input_url = base_url + filename
        if should_skip(input_url, output_filename, size):
            continue
        future = pool.apply_async(process_checksum_file, (gridftp_url, output_filename))
        future.filename = filename
        future.size = size
        futures.append(future)
        #break
    pool.close()
    while futures:
        timed_out_futures = []
        for future in futures:
            try:
                future.get(180)
            except multiprocessing.TimeoutError:
                print "Timed out when waiting for file %s; will retry later." % future.filename
                timed_out_futures.append(future)
            except Exception as e:
                print "Failure when trying to checksum %s (size %d); will regenerate.  Exception: %s" % (future.filename, future.size, str(e))
                #broken_filenames.append((future.filename, future.size))
        futures = timed_out_futures
    print "All jobs processed - closing up pool."
    pool.join()
    filenames = broken_filenames


def process_dir(base_url, directory, ignore=None):
    fs = XRootD.client.FileSystem(base_url)
    worklist = [directory]
    filelist = []
    base_len = len(directory)
    while worklist:
        cwd = worklist.pop()
        print "Processing", cwd
        status, dirlist = fs.dirlist("/" + cwd,
                                    flags=XRootD.client.flags.DirListFlags.STAT)
        if status.status:
            print "Failed to list directory", cwd, "skipping"
            print status
            continue
        for entry in dirlist.dirlist:
            if entry.statinfo.flags & XRootD.client.flags.StatInfoFlags.IS_DIR:
                should_ignore = False
                if ignore:
                    for ignore_rule in ignore.split(","):
                        if fnmatch.fnmatch(entry.name, ignore_rule):
                            should_ignore = True
                            break
                if not should_ignore:
                    worklist.append(cwd + "/" + entry.name)
            elif not ignore:
                fname = cwd + "/" + entry.name
                filelist.append((fname[base_len:], entry.statinfo.size))
            else:
                should_ignore = False
                if ignore:
                    for ignore_rule in ignore.split(","):
                         if fnmatch.fnmatch(entry.name, ignore_rule):
                             should_ignore = True
                             break
                if not should_ignore:
                    fname = cwd + "/" + entry.name
                    filelist.append((fname[base_len:], entry.statinfo.size))
    return filelist


def main():
    opts, args = parse_opts()
    dest = args[-1]
    srcs = args[:-1]
    print "Inputs to process:", ",".join(srcs)
    concurrency = opts.concurrency
    starttime = time.time()

    for src in srcs:
        source_filelist = []
        src, gridftp_src = src.split(",")
        src_parsed = urlparse.urlparse(src)
        base_url = src_parsed.scheme + "://" + src_parsed.netloc
        directory = src_parsed.path
        filelist = process_dir(base_url, directory, ignore=opts.ignore)
        print "Found %d files to process" % len(filelist)
        process_files(filelist, src, gridftp_src, dest, concurrency)

    processing_time = time.time() - starttime
    print "Total of %d bytes processed in %.2f seconds." % (g_bytes_xfer, processing_time)
    print "Processing rate: %.2fMB/s" % (g_bytes_xfer/processing_time/(1024*1024.))

    if not opts.failed:
        if g_failed_files:
            print "Failed files:"
            for file in g_failed_files:
                print file
    else:
        fh = open(opts.failed, "w")
        fh.write("\n".join(g_failed_files))
    if not g_failed_files:
        print "Synchronization completed successfully!"
        return 0
    else:
        print "There were %d synchronization failures." % len(g_failed_files)
        # TODO: cleanup partial grafts
        return 1


if __name__ == '__main__':
    sys.exit(main())


