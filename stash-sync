#!/usr/bin/python

import os
import re
import sys
import time
import errno
import signal
import optparse
import urlparse
import threading

oldpath = sys.path
sys.path = sys.path[1:]

import XRootD.client

sys.path = oldpath

g_failed_files = []
g_processed_files = []
g_bytes_xfer = 0
g_bytes_xfer_lock = threading.Lock()

g_starttime = time.time()


def inc_bytes(count):
    global g_bytes_xfer
    g_bytes_xfer_lock.acquire()
    try:
        g_bytes_xfer += count
    finally:
        g_bytes_xfer_lock.release()

def parse_opts():
    parser = optparse.OptionParser(usage="%prog [options] src1 [src2 ...] dest")
    parser.add_option("-n", "--concurrency", dest="concurrency",
                       help="Number of helper processes", default=4,
                       type="int")
    parser.add_option("-f", "--failed", dest="failed",
                      help="Output filename for list of failed files")
    parser.add_option("-t", "--max-time", dest="max_time", type="int",
                      help="Time threshold for starting new batches "
                           " of transfers.  Specified in minutes",
                      default = 0)
    parser.add_option("-d", "--delete", dest="delete", action="store_true",
                      help="Delete files from CVMFS if they are not present in Stash")

    opts, args = parser.parse_args()
    if len(args) < 2:
        print >> sys.stderr, "Need at least one source and destination"
        parser.print_usage()
        sys.exit(1)

    return opts, args


def create_read_callback(offset, readfp, writefd, filename, pid):
    def read_callback(status, response, hostlist):
        # NOTE: due to mutex ordering issues, we CANNOT invoke xrootd
        # functions (including letting objects fall out of scope).
        #print "Read callback for %s, offset %d, response" % (filename, offset)
        if status.status:
            print "Read callback failed for %s:" % filename, status
            print "Hostlist:", hostlist
            for host in hostlist.hosts:
                print "-", host.url
            g_failed_files.append(filename)
            os.kill(pid, signal.SIGKILL)
            # In this case, response is None; close and fail out.
        if (response == None) or (len(response) == 0):
            print "Finished processing", filename
            os.close(writefd)
        else:
            inc_bytes(len(response))
            try:
                os.write(writefd, response)
            except OSError, oe:
                if oe.errno != errno.EPIPE:
                    raise
                print "cvmfs_swissknife process unexpectedly died."
                g_failed_files.append(filename)
                return
            next_offset = offset + len(response)
            kwargs = {'offset': next_offset,
                      'size': 1024*1024,
                      'callback': create_read_callback(next_offset, readfp,
                                                writefd, filename, pid)
                     }
            status = readfp.read(**kwargs)
            if status.status:
                print "Failed to create callback for", filename
                g_failed_files.append(filename)
                os.kill(pid, signal.SIGKILL)

    return read_callback


def kickoff_graft(fp, input_url, output_filename):
    readfp, writefp = os.pipe()
    pid = os.fork()
    if not pid:
        try:
            os.dup2(readfp, 0)
            os.close(writefp)
            args = ['cvmfs_swissknife', 'graft', '-i', '-', '-o', output_filename, '-Z', 'none']
            os.execvp("cvmfs_swissknife", args)
        finally:
            print "Failed to exec cvmfs_swissknife for processing", input_url
            os._exit(1)
    os.close(readfp)
    callback = create_read_callback(0, fp, writefp, input_url, pid)
    status = fp.read(offset=0, size=1024*1024, callback=callback)
    if status.status:
        print "Failed to create read request for", input_url
        print "Status:", status
        print "Killing graft PID", pid
        os.kill(pid, signal.SIGKILL)
        g_failed_files.append(input_url)
        return False
    return True


def process_files(filenames, base_url, output_dir, count, max_time):
    while filenames:
        chunk_filenames = filenames[:500]
        filenames = filenames[500:]
        process_files_impl(chunk_filenames, base_url, output_dir, count)
        if (max_time > 0) and ((time.time() - g_starttime) > max_time*60):
            break


def process_files_impl(filenames, base_url, output_dir, count):
    current = 0
    worklist = []

    files_opened = 0
    # Serialize on opening files
    for filename, size in filenames:
        input_url = base_url + filename
        output_filename = output_dir + filename
        if os.path.exists(output_filename):
            st = os.stat(output_filename)
            if size == st.st_size:
                #print "Skipping file %s as %s already exists." % \
                #    (input_url, output_filename)
                continue
            else:
                print "Size mismatch!  Regenerating file %s (size %d) -> %s (size %d)" % \
                    (input_url, size, output_filename, st.st_size)
                os.unlink(output_filename)
        try:
            os.makedirs(os.path.split(output_filename)[0])
        except OSError, oe:
            if oe.errno != errno.EEXIST:
                raise
        fp = XRootD.client.File()
        status = fp.open(input_url)[0]
        if status.status:
            g_failed_files.append(input_url)
            print "Failed to open file %s:" % input_url, status
        else:
            worklist.append((filename, fp))
            files_opened += 1

    # Process in parallel.
    try:
        for filename, fp in worklist:
            input_url = base_url + filename
            output_filename = output_dir + filename
            if current == count:
                status = os.wait()[1]
                current -= 1
                if status:
                    print "Failed to process file; graft creation exited with error", status
            print "Processing file %s -> %s" % (input_url, output_filename)
            kickoff_graft(fp, input_url, output_filename)
            current += 1
    finally:
        # Process tails - must do this before exceptions are propagated as we
        # may deadlock if there are callbacks while we destroy the open file
        # handles.
        while current:
            status = os.wait()[1]
            current -= 1
            if status:
                print "Failed to process file; graft creation exited with error", status

        # A reference in the worklist, to sys.getrefcount, and the loop variable.
        # If there is an additional ref, there must be an outstanding callback.
        for filename, fp in worklist:
            while sys.getrefcount(fp) > 3:
                print filename, sys.getrefcount(fp)
                time.sleep(.2)


user_dir_regexp = re.compile(r'^/+user/+([A-Za-z0-9.]+)/*$')
def process_dir(base_url, directory):
    fs = XRootD.client.FileSystem(base_url)
    worklist = [directory]
    filelist = []
    failed_list = []
    base_len = len(directory)
    while worklist:
        cwd = worklist.pop()
        if user_dir_regexp.match(cwd):
            cwd += "/public"
        print "Processing", cwd
        status, dirlist = fs.dirlist("/" + cwd,
                                    flags=XRootD.client.flags.DirListFlags.STAT)
        if status.status:
            print "Failed to list directory:", cwd
            failed_list.append("/" + cwd)
            continue
        for entry in dirlist.dirlist:
            if entry.statinfo.flags & XRootD.client.flags.StatInfoFlags.IS_DIR:
                worklist.append(cwd + "/" + entry.name)
            else:
                fname = cwd + "/" + entry.name
                filelist.append((fname[base_len:], entry.statinfo.size))
    return filelist, failed_list


def process_deletions(base_dir, orig_filelist, failed, dryrun):
    slash_re = re.compile("/+")
    filelist = []
    for fname in orig_filelist:
        try:
            filelist.append(slash_re.subn("/", base_dir + "/" + fname[0])[0])
        except TypeError:
            print fname
            raise
    filelist = set(filelist)
    dirlist = set([base_dir])
    for fname in filelist:
        dirname = os.path.split(fname)[0]
        while dirname not in dirlist:
            dirlist.add(dirname)
            dirname = os.path.split(dirname)[0]
    failed = set([slash_re.subn("/", i)[0] for i in failed])
    count = 0
    for dirpath, dirs, files in os.walk(base_dir):
        dirpath = slash_re.sub("/", "/" + dirpath)
        found_bad = False
        for badpath in failed:
            if dirpath.startswith(badpath):
                found_bad = True
                break
        if found_bad:
            continue
        for fname in files:
            if fname.startswith(".cvmfs"):
                continue
            full_fname = os.path.join(dirpath, fname)
            if full_fname not in filelist:
                count += 1
                if dryrun:
                    print "Would remove file %s." % full_fname
                else:
                    os.unlink(full_fname)
        for dname in dirs:
            full_dname = os.path.join(dirpath, dname)
            if full_dname not in dirlist:
                if dryrun:
                    print "Would remove directory %s." % full_dname
                else:
                    # OverlayFS returns errant ENOTEMPTY on RHEL7.2
                    try:
                        os.rmdir(full_dname)
                    except OSError, oe:
                        if os.errno != errno.ENOTEMPTY:
                            raise
    if count and dryrun:
        print "Would have removed %d files." % count


def main():
    opts, args = parse_opts()
    dest = args[-1]
    srcs = args[:-1]
    print "Inputs to process:", ",".join(srcs)
    concurrency = opts.concurrency
    starttime = time.time()

    full_filelist = []
    full_failed_list = []
    for src in srcs:
        source_filelist = []
        src_parsed = urlparse.urlparse(src)
        base_url = src_parsed.scheme + "://" + src_parsed.netloc
        directory = src_parsed.path
        filelist, failed_list = process_dir(base_url, directory)
        full_filelist += filelist
        full_failed_list += failed_list
        print "Found %d files to process" % len(filelist)
        process_files(filelist, src, dest, concurrency, opts.max_time)

    processing_time = time.time() - starttime
    print "Total of %d bytes processed in %.2f seconds." % (g_bytes_xfer, processing_time)
    print "Processing rate: %.2fMB/s" % (g_bytes_xfer/processing_time/(1024*1024.))

    dryrun = False
    if opts.delete or dryrun:
        process_deletions(dest, full_filelist, full_failed_list, dryrun)

    if not opts.failed:
        if g_failed_files:
            print "Failed files:"
            for file in g_failed_files:
                print file
    else:
        fh = open(opts.failed, "w")
        fh.write("\n".join(g_failed_files))
    if not g_failed_files:
        print "Synchronization completed successfully!"
        return 0
    else:
        print "There were %d synchronization failures." % len(g_failed_files)
        # TODO: cleanup partial grafts
        return 1


if __name__ == '__main__':
    sys.exit(main())


